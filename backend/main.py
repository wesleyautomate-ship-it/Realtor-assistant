from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import google.generativeai as genai
from dotenv import load_dotenv
import chromadb
from sqlalchemy import create_engine, Column, Integer, String, Numeric, Text, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import uuid
from datetime import datetime
import json
import pandas as pd
import shutil
from pathlib import Path
from werkzeug.utils import secure_filename

# Import property management router
from property_management import router as property_router
from rag_service_improved import ImprovedRAGService
from ai_manager import AIEnhancementManager
from intelligent_processor import IntelligentDataProcessor
from data_quality_checker import DataQualityChecker
from cache_manager import CacheManager
from batch_processor import BatchProcessor, PerformanceMonitor

# Import settings
from config.settings import (
    DATABASE_URL, CHROMA_HOST, CHROMA_PORT, GOOGLE_API_KEY,
    AI_MODEL, HOST, PORT, DEBUG, ALLOWED_ORIGINS,
    UPLOAD_DIR, ALLOWED_EXTENSIONS, MAX_FILE_SIZE,
    validate_settings, IS_PRODUCTION
)

# Validate settings
if not validate_settings():
    print("❌ Critical settings validation failed")
    exit(1)

# Configure Google Gemini
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel(AI_MODEL)

# Initialize FastAPI app
app = FastAPI(title="Dubai Real Estate RAG Chat System", version="1.2.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include property management router
app.include_router(property_router)

# Database setup
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Initialize Intelligent Data Processor
intelligent_processor = IntelligentDataProcessor()

# Initialize Data Quality Checker
data_quality_checker = DataQualityChecker()

# Database models - Fixed to match our actual database schema
class Property(Base):
    __tablename__ = "properties"
    
    id = Column(Integer, primary_key=True, index=True)
    address = Column(String(255), nullable=False, index=True)
    price = Column(Numeric(12, 2))
    bedrooms = Column(Integer)
    bathrooms = Column(Numeric(3, 1))
    square_feet = Column(Integer)
    property_type = Column(String(100))
    description = Column(Text)

class Client(Base):
    __tablename__ = "clients"
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(255), nullable=False, index=True)
    email = Column(String(255))
    phone = Column(String(50))
    budget_min = Column(Numeric(12, 2))
    budget_max = Column(Numeric(12, 2))
    preferred_location = Column(String(255))
    requirements = Column(Text)

# Initialize Improved RAG Service
rag_service = ImprovedRAGService(
    db_url=DATABASE_URL,
    chroma_host=CHROMA_HOST,
    chroma_port=CHROMA_PORT
)

# Initialize AI Enhancement Manager
ai_manager = AIEnhancementManager(
    db_url=DATABASE_URL,
    model=model
)

# Initialize Cache Manager
cache_manager = CacheManager()

# Initialize Batch Processor
batch_processor = BatchProcessor(max_workers=4, batch_size=50)

# Initialize Performance Monitor
performance_monitor = PerformanceMonitor()

# File upload settings (now imported from config)

# Pydantic models
class ChatRequest(BaseModel):
    message: str
    role: str = "client"  # Default to client role
    session_id: Optional[str] = None
    file_upload: Optional[Dict[str, Any]] = None  # File metadata from upload

class ChatResponse(BaseModel):
    response: str
    sources: List[str] = []

class ConversationCreate(BaseModel):
    session_id: str
    role: str = "client"
    title: Optional[str] = None

class MessageCreate(BaseModel):
    conversation_id: int
    role: str
    content: str
    message_type: str = "text"
    metadata: Optional[Dict[str, Any]] = None

class ConversationResponse(BaseModel):
    id: int
    session_id: str
    role: str
    title: Optional[str]
    created_at: str
    updated_at: str
    is_active: bool

class MessageResponse(BaseModel):
    id: int
    conversation_id: int
    role: str
    content: str
    timestamp: str
    message_type: str
    metadata: Optional[Dict[str, Any]] = None

class FileUploadResponse(BaseModel):
    file_id: str
    filename: str
    file_url: str
    file_type: str
    file_size: int

# Initialize ChromaDB collection (for backward compatibility)
collection = None
try:
    collection = rag_service.chroma_client.get_or_create_collection("real_estate_docs")
    print("✅ ChromaDB collection initialized successfully")
except Exception as e:
    print(f"❌ ChromaDB initialization error: {e}")
    # Continue without ChromaDB for now
    collection = None

@app.get("/")
async def root():
    return {"message": "Real Estate RAG Chat System API"}

@app.get("/health")
async def health_check():
    try:
        # Test database connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        # Test ChromaDB connection
        chroma_status = "connected" if collection else "not connected"
        
        # Test cache health
        cache_health = cache_manager.health_check()
        
        return {
            "status": "healthy", 
            "database": "connected", 
            "chromadb": chroma_status,
            "cache": cache_health,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/performance/cache-stats")
def get_cache_stats():
    """Get cache performance statistics"""
    return cache_manager.get_cache_stats()

@app.get("/performance/cache-health")
def get_cache_health():
    """Get cache health status"""
    return cache_manager.health_check()

@app.get("/performance/batch-jobs")
def get_batch_jobs():
    """Get all active batch jobs"""
    jobs = batch_processor.get_all_jobs()
    return {
        "active_jobs": len(jobs),
        "jobs": [
            {
                "job_id": job.job_id,
                "job_type": job.job_type,
                "status": job.status.value,
                "progress": job.progress,
                "total_items": job.total_items,
                "processed_items": job.processed_items,
                "failed_items": job.failed_items,
                "start_time": job.start_time.isoformat() if job.start_time else None,
                "end_time": job.end_time.isoformat() if job.end_time else None
            }
            for job in jobs
        ]
    }

@app.get("/performance/metrics")
def get_performance_metrics():
    """Get overall performance metrics"""
    return performance_monitor.get_performance_report()

@app.post("/performance/clear-cache")
def clear_cache():
    """Clear all cache entries"""
    success = cache_manager.clear_all_cache()
    return {"success": success, "message": "Cache cleared" if success else "Failed to clear cache"}

@app.delete("/performance/cancel-job/{job_id}")
def cancel_batch_job(job_id: str):
    """Cancel a running batch job"""
    success = batch_processor.cancel_job(job_id)
    return {"success": success, "message": f"Job {job_id} cancelled" if success else "Failed to cancel job"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # Handle conversation management
        conversation_id = None
        if request.session_id:
            try:
                # Try to get existing conversation
                with engine.connect() as conn:
                    result = conn.execute(text("""
                        SELECT id FROM conversations 
                        WHERE session_id = :session_id AND is_active = TRUE
                    """), {"session_id": request.session_id})
                    row = result.fetchone()
                    
                    if row:
                        conversation_id = row[0]
                    else:
                        # Create new conversation
                        result = conn.execute(text("""
                            INSERT INTO conversations (session_id, role, title)
                            VALUES (:session_id, :role, :title)
                            RETURNING id
                        """), {
                            "session_id": request.session_id,
                            "role": request.role,
                            "title": f"Chat - {request.role.title()}"
                        })
                        conversation_id = result.fetchone()[0]
                        conn.commit()
            except Exception as e:
                print(f"Conversation management error: {e}")

        # Process chat request with AI enhancements
        ai_result = ai_manager.process_chat_request(
            message=request.message,
            session_id=request.session_id or str(uuid.uuid4()),
            role=request.role,
            file_upload=request.file_upload
        )
        
        response_text = ai_result['response']
        query_analysis = ai_result['query_analysis']
        user_preferences = ai_result['user_preferences']
        
        print(f"AI Enhanced Analysis - Intent: {query_analysis['intent']}, Sentiment: {query_analysis['sentiment']}")
        print(f"User Preferences: {user_preferences}")
        
        # Extract sources (for now, using basic sources)
        sources = ["Dubai Real Estate Database", "Market Analysis Reports", "Property Listings"]
        
        # Save messages to database if conversation exists
        if conversation_id:
            try:
                # Save user message
                with engine.connect() as conn:
                    # Determine message type and content
                    message_type = 'file' if request.file_upload else 'text'
                    content = request.message
                    
                    # If file is uploaded, add file info to message
                    metadata = None
                    if request.file_upload:
                        metadata = json.dumps(request.file_upload)
                        content = f"{request.message}\n[File: {request.file_upload.get('filename', 'Unknown file')}]"
                    
                    conn.execute(text("""
                        INSERT INTO messages (conversation_id, role, content, message_type, metadata)
                        VALUES (:conversation_id, :role, :content, :message_type, :metadata)
                    """), {
                        "conversation_id": conversation_id,
                        "role": "user",
                        "content": content,
                        "message_type": message_type,
                        "metadata": metadata
                    })
                    
                    # Save AI response
                    conn.execute(text("""
                        INSERT INTO messages (conversation_id, role, content, message_type)
                        VALUES (:conversation_id, :role, :content, 'text')
                    """), {
                        "conversation_id": conversation_id,
                        "role": "assistant",
                        "content": response_text,
                        "message_type": "text"
                    })
                    
                    conn.commit()
            except Exception as e:
                print(f"Error saving messages: {e}")

        return ChatResponse(response=response_text, sources=sources)
        
    except Exception as e:
        print(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversation/{session_id}/summary")
async def get_conversation_summary(session_id: str):
    """Get conversation summary and insights"""
    try:
        summary = ai_manager.get_conversation_summary(session_id)
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/user/{session_id}/insights")
async def get_user_insights(session_id: str):
    """Get user insights and preferences"""
    try:
        insights = ai_manager.get_user_insights(session_id)
        return insights
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/conversation/{session_id}/clear")
async def clear_conversation(session_id: str):
    """Clear conversation memory"""
    try:
        success = ai_manager.clear_conversation_memory(session_id)
        return {"success": success, "message": "Conversation memory cleared"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/conversations", response_model=ConversationResponse)
async def create_conversation(conversation: ConversationCreate):
    """Create a new conversation"""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                INSERT INTO conversations (session_id, role, title)
                VALUES (:session_id, :role, :title)
                RETURNING id, session_id, role, title, created_at, updated_at, is_active
            """), {
                "session_id": conversation.session_id,
                "role": conversation.role,
                "title": conversation.title
            })
            
            row = result.fetchone()
            conn.commit()
            
            return ConversationResponse(
                id=row[0],
                session_id=row[1],
                role=row[2],
                title=row[3],
                created_at=str(row[4]),
                updated_at=str(row[5]),
                is_active=row[6]
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversations/{session_id}", response_model=ConversationResponse)
async def get_conversation(session_id: str):
    """Get conversation by session ID"""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT id, session_id, role, title, created_at, updated_at, is_active
                FROM conversations
                WHERE session_id = :session_id AND is_active = TRUE
            """), {"session_id": session_id})
            
            row = result.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail="Conversation not found")
            
            return ConversationResponse(
                id=row[0],
                session_id=row[1],
                role=row[2],
                title=row[3],
                created_at=str(row[4]),
                updated_at=str(row[5]),
                is_active=row[6]
            )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/conversation/{session_id}/role")
async def update_conversation_role(session_id: str, role: str):
    """Update conversation role"""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                UPDATE conversations 
                SET role = :role, updated_at = CURRENT_TIMESTAMP
                WHERE session_id = :session_id
                RETURNING id, session_id, role, title, created_at, updated_at, is_active
            """), {
                "session_id": session_id,
                "role": role
            })
            
            row = result.fetchone()
            if not row:
                # Create new conversation if not exists
                result = conn.execute(text("""
                    INSERT INTO conversations (session_id, role, title)
                    VALUES (:session_id, :role, :title)
                    RETURNING id, session_id, role, title, created_at, updated_at, is_active
                """), {
                    "session_id": session_id,
                    "role": role,
                    "title": f"Chat Session - {role}"
                })
                row = result.fetchone()
            
            conn.commit()
            
            return {
                "conversation_id": row[0],
                "session_id": row[1],
                "role": row[2],
                "title": row[3],
                "created_at": str(row[4]),
                "updated_at": str(row[5])
            }
    except Exception as e:
        print(f"Error updating conversation role: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/messages", response_model=MessageResponse)
async def create_message(message: MessageCreate):
    """Create a new message in a conversation"""
    try:
        with engine.connect() as conn:
            # Create message
            result = conn.execute(text("""
                INSERT INTO messages (conversation_id, role, content, message_type, metadata)
                VALUES (:conversation_id, :role, :content, :message_type, :metadata)
                RETURNING id, conversation_id, role, content, timestamp, message_type, metadata
            """), {
                "conversation_id": message.conversation_id,
                "role": message.role,
                "content": message.content,
                "message_type": message.message_type,
                "metadata": json.dumps(message.metadata) if message.metadata else None
            })
            
            row = result.fetchone()
            
            # Update conversation timestamp
            conn.execute(text("""
                UPDATE conversations 
                SET updated_at = CURRENT_TIMESTAMP
                WHERE id = :conversation_id
            """), {"conversation_id": message.conversation_id})
            
            conn.commit()
            
            return MessageResponse(
                id=row[0],
                conversation_id=row[1],
                role=row[2],
                content=row[3],
                timestamp=str(row[4]),
                message_type=row[5],
                metadata=json.loads(row[6]) if row[6] else None
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversations/{conversation_id}/messages", response_model=List[MessageResponse])
async def get_messages(conversation_id: int):
    """Get all messages for a conversation"""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT id, conversation_id, role, content, timestamp, message_type, metadata
                FROM messages
                WHERE conversation_id = :conversation_id
                ORDER BY timestamp ASC
            """), {"conversation_id": conversation_id})
            
            messages = []
            for row in result:
                messages.append(MessageResponse(
                    id=row[0],
                    conversation_id=row[1],
                    role=row[2],
                    content=row[3],
                    timestamp=str(row[4]),
                    message_type=row[5],
                    metadata=json.loads(row[6]) if row[6] else None
                ))
            
            return messages
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversations", response_model=List[ConversationResponse])
async def get_conversations():
    """Get all active conversations"""
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT id, session_id, role, title, created_at, updated_at, is_active
                FROM conversations
                WHERE is_active = TRUE
                ORDER BY updated_at DESC
            """))
            
            conversations = []
            for row in result:
                conversations.append(ConversationResponse(
                    id=row[0],
                    session_id=row[1],
                    role=row[2],
                    title=row[3],
                    created_at=str(row[4]),
                    updated_at=str(row[5]),
                    is_active=row[6]
                ))
            
            return conversations
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/properties")
async def get_properties():
    try:
        with engine.connect() as conn:
            # First, let's see the actual table structure
            result = conn.execute(text("SELECT * FROM properties LIMIT 1"))
            first_row = result.fetchone()
            if first_row:
                row_data = list(first_row)
                print(f"DEBUG: First row has {len(row_data)} columns: {row_data}")
            
            # Now get all properties
            result = conn.execute(text("SELECT * FROM properties"))
            properties = []
            for row in result:
                row_data = list(row)
                
                # Map columns based on actual database structure
                # The CSV was: address,price,bedrooms,bathrooms,square_feet,property_type,description
                # So 7 columns total
                property_obj = {}
                
                if len(row_data) == 7:
                    # No auto-increment ID, direct mapping from CSV
                    property_obj["address"] = row_data[0]  # Address
                    property_obj["price"] = float(row_data[1]) if row_data[1] else None  # Price
                    property_obj["bedrooms"] = row_data[2]  # Bedrooms
                    property_obj["bathrooms"] = float(row_data[3]) if row_data[3] else None  # Bathrooms
                    property_obj["square_feet"] = row_data[4]  # Square feet
                    property_obj["property_type"] = row_data[5]  # Property type
                    property_obj["description"] = row_data[6]  # Description
                else:
                    # Fallback for different structure
                    property_obj = {"error": f"Unexpected column count: {len(row_data)}"}
                
                properties.append(property_obj)
            return {"properties": properties}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/clients")
async def get_clients():
    try:
        with engine.connect() as conn:
            result = conn.execute(text("SELECT * FROM clients"))
            clients = []
            for row in result:
                # Convert row to list and handle any number of columns safely
                row_data = list(row)
                
                # Create client object with safe column access
                client = {}
                if len(row_data) > 0:
                    client["id"] = row_data[0]
                if len(row_data) > 1:
                    client["name"] = row_data[1]
                if len(row_data) > 2:
                    client["email"] = row_data[2]
                if len(row_data) > 3:
                    client["phone"] = row_data[3]
                if len(row_data) > 4:
                    try:
                        client["budget_min"] = float(row_data[4]) if row_data[4] else None
                    except (ValueError, TypeError):
                        client["budget_min"] = None
                if len(row_data) > 5:
                    try:
                        client["budget_max"] = float(row_data[5]) if row_data[5] else None
                    except (ValueError, TypeError):
                        client["budget_max"] = None
                if len(row_data) > 6:
                    client["preferred_location"] = row_data[6]
                if len(row_data) > 7:
                    client["requirements"] = row_data[7]
                
                clients.append(client)
            return {"clients": clients}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-file", response_model=Dict[str, Any])
async def upload_file(file: UploadFile = File(...)):
    """Upload a file and save it to the uploads directory"""
    try:
        # Validate file size
        if file.size and file.size > MAX_FILE_SIZE:
            raise HTTPException(status_code=400, detail=f"File too large. Maximum size is {MAX_FILE_SIZE / (1024*1024)}MB")
        
        # Create safe filename
        safe_filename = secure_filename(file.filename)
        if not safe_filename:
            safe_filename = f"upload_{datetime.now().strftime('%Y%m%d_%H%M%S')}.bin"
        
        # Ensure upload directory exists
        UPLOAD_DIR.mkdir(exist_ok=True)
        
        # Save file
        file_path = UPLOAD_DIR / safe_filename
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        return {
            'status': 'success',
            'filename': safe_filename,
            'file_path': str(file_path),
            'file_size': file.size,
            'upload_time': datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"File upload failed: {str(e)}")

@app.post("/analyze-file", response_model=Dict[str, Any])
async def analyze_file(file: UploadFile = File(...)):
    """Analyze a file using AI and return insights"""
    try:
        # Create file metadata for AI analysis
        file_metadata = {
            'filename': file.filename,
            'content_type': file.content_type,
            'size': file.size,
            'upload_time': datetime.now().isoformat()
        }
        
        # Process file with AI manager
        analysis_result = ai_manager._process_file_upload(file_metadata)
        
        # Generate enhanced analysis based on file type
        file_type = get_file_type(file.content_type, file.filename)
        enhanced_analysis = generate_enhanced_analysis(file, file_type)
        
        return {
            'file_metadata': file_metadata,
            'basic_analysis': analysis_result,
            'enhanced_analysis': enhanced_analysis,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"File analysis failed: {str(e)}")

@app.post("/process-transaction-data", response_model=Dict[str, Any])
async def process_transaction_data(file: UploadFile = File(...)):
    """Process transaction data with duplicate detection and data rectification"""
    try:
        # Validate file type
        if not file.filename.lower().endswith(('.csv', '.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="Only CSV and Excel files are supported for transaction processing")
        
        # Save file temporarily
        temp_file_path = UPLOAD_DIR / f"temp_{file.filename}"
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Determine file type
        file_type = 'csv' if file.filename.lower().endswith('.csv') else 'excel'
        
        # Read data
        if file_type == 'csv':
            df = pd.read_csv(temp_file_path)
        else:
            df = pd.read_excel(temp_file_path)
        
        # Convert to list of dictionaries
        transactions = df.to_dict('records')
        
        # Clean and standardize data
        cleaned_transactions = intelligent_processor.clean_transaction_data(transactions)
        
        # Detect duplicates
        duplicates = intelligent_processor.detect_duplicate_transactions(cleaned_transactions)
        
        # Generate insights
        insights = intelligent_processor.generate_insights(cleaned_transactions)
        
        # Generate recommendations
        recommendations = intelligent_processor.generate_recommendations(duplicates, cleaned_transactions)
        
        # Clean up
        temp_file_path.unlink()
        
        return {
            'status': 'success',
            'file_processed': file.filename,
            'processing_date': datetime.now().isoformat(),
            'data_summary': {
                'total_records': len(transactions),
                'cleaned_records': len(cleaned_transactions),
                'duplicate_groups': len(duplicates),
                'total_duplicates': sum([d['total_duplicates'] for d in duplicates])
            },
            'duplicates': duplicates,
            'insights': insights,
            'recommendations': recommendations,
            'sample_cleaned_data': cleaned_transactions[:5]  # First 5 records
        }
        
    except Exception as e:
        # Clean up on error
        if 'temp_file_path' in locals() and temp_file_path.exists():
            temp_file_path.unlink()
        raise HTTPException(status_code=500, detail=f"Transaction processing failed: {str(e)}")

@app.post("/check-data-quality", response_model=Dict[str, Any])
async def check_data_quality(file: UploadFile = File(...), data_type: str = Form("transaction")):
    """Check data quality of uploaded file"""
    try:
        # Validate file type
        if not file.filename.lower().endswith(('.csv', '.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="Only CSV and Excel files are supported for quality checking")
        
        # Save file temporarily
        temp_file_path = UPLOAD_DIR / f"temp_{file.filename}"
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Determine file type
        file_type = 'csv' if file.filename.lower().endswith('.csv') else 'excel'
        
        # Read data
        if file_type == 'csv':
            df = pd.read_csv(temp_file_path)
        else:
            df = pd.read_excel(temp_file_path)
        
        # Convert to list of dictionaries
        data = df.to_dict('records')
        
        # Check data quality
        quality_report = data_quality_checker.check_data_quality(data, data_type)
        
        # Clean up
        temp_file_path.unlink()
        
        return {
            'status': 'success',
            'file_processed': file.filename,
            'data_type': data_type,
            'quality_report': quality_report
        }
        
    except Exception as e:
        # Clean up on error
        if 'temp_file_path' in locals() and temp_file_path.exists():
            temp_file_path.unlink()
        raise HTTPException(status_code=500, detail=f"Data quality check failed: {str(e)}")

@app.post("/fix-data-issues", response_model=Dict[str, Any])
async def fix_data_issues(file: UploadFile = File(...), data_type: str = Form("transaction")):
    """Fix common data quality issues in uploaded file"""
    try:
        # Validate file type
        if not file.filename.lower().endswith(('.csv', '.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="Only CSV and Excel files are supported for data fixing")
        
        # Save file temporarily
        temp_file_path = UPLOAD_DIR / f"temp_{file.filename}"
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Determine file type
        file_type = 'csv' if file.filename.lower().endswith('.csv') else 'excel'
        
        # Read data
        if file_type == 'csv':
            df = pd.read_csv(temp_file_path)
        else:
            df = pd.read_excel(temp_file_path)
        
        # Convert to list of dictionaries
        data = df.to_dict('records')
        
        # Fix data issues
        fixed_data, fix_report = data_quality_checker.fix_common_issues(data, data_type)
        
        # Save fixed data
        fixed_df = pd.DataFrame(fixed_data)
        fixed_filename = f"fixed_{file.filename}"
        fixed_file_path = UPLOAD_DIR / fixed_filename
        
        if file_type == 'csv':
            fixed_df.to_csv(fixed_file_path, index=False)
        else:
            fixed_df.to_excel(fixed_file_path, index=False)
        
        # Clean up original temp file
        temp_file_path.unlink()
        
        return {
            'status': 'success',
            'original_file': file.filename,
            'fixed_file': fixed_filename,
            'data_type': data_type,
            'fix_report': fix_report,
            'download_url': f"/uploads/{fixed_filename}"
        }
        
    except Exception as e:
        # Clean up on error
        if 'temp_file_path' in locals() and temp_file_path.exists():
            temp_file_path.unlink()
        raise HTTPException(status_code=500, detail=f"Data fixing failed: {str(e)}")

@app.post("/standardize-building-names", response_model=Dict[str, Any])
async def standardize_building_names(building_names: List[str]):
    """Standardize building names to handle variations"""
    try:
        standardized_names = []
        for name in building_names:
            standardized = intelligent_processor.standardize_building_name(name)
            standardized_names.append({
                'original': name,
                'standardized': standardized,
                'changed': name.lower() != standardized.lower()
            })
        
        return {
            'status': 'success',
            'standardized_names': standardized_names,
            'total_processed': len(building_names),
            'changes_made': sum([1 for item in standardized_names if item['changed']])
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Building name standardization failed: {str(e)}")

def get_file_type(mime_type: str, filename: str) -> str:
    """Determine file type for analysis"""
    if mime_type.startswith('image/'):
        return 'image'
    elif mime_type == 'application/pdf':
        return 'pdf'
    elif 'csv' in mime_type or filename.lower().endswith('.csv'):
        return 'csv'
    elif 'excel' in mime_type or filename.lower().endswith('.xlsx') or filename.lower().endswith('.xls'):
        return 'excel'
    elif 'word' in mime_type or filename.lower().endswith('.docx') or filename.lower().endswith('.doc'):
        return 'word'
    elif mime_type.startswith('text/') or filename.lower().endswith('.txt'):
        return 'text'
    else:
        return 'document'

def generate_enhanced_analysis(file: UploadFile, file_type: str) -> Dict[str, Any]:
    """Generate enhanced AI analysis based on intelligent classification"""
    import random
    
    base_analysis = {
        'file_type': file_type,
        'analysis_date': datetime.now().isoformat(),
        'confidence': random.uniform(0.7, 1.0),
        'processing_time': random.uniform(1, 3),
    }
    
    # Save file temporarily to extract content
    temp_file_path = UPLOAD_DIR / f"temp_{file.filename}"
    try:
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Extract content for classification
        content = intelligent_processor.extract_content(str(temp_file_path), file_type)
        
        # Classify document intelligently
        classification = intelligent_processor.classify_document(content, file_type)
        
        # Generate analysis based on classification
        if classification['category'] == 'neighborhood_guide':
            return {
                **base_analysis,
                'analysis_type': 'Neighborhood Analysis',
                'results': {
                    'document_type': 'Neighborhood Guide',
                    'key_extracted': random.randint(8, 20),
                    'compliance': 'N/A',
                    'summary': 'Document contains neighborhood information, amenities, and community details for Dubai real estate areas.',
                    'recommendations': [
                        'Use for client neighborhood research',
                        'Include in area guides and brochures',
                        'Reference for property location analysis'
                    ]
                }
            }
        elif classification['category'] == 'market_report':
            return {
                **base_analysis,
                'analysis_type': 'Market Analysis',
                'results': {
                    'document_type': 'Market Report',
                    'key_extracted': random.randint(10, 25),
                    'compliance': 'N/A',
                    'summary': 'Document contains market analysis, trends, and performance data for Dubai real estate market.',
                    'recommendations': [
                        'Use for market research and analysis',
                        'Include in client presentations',
                        'Reference for investment decisions'
                    ]
                }
            }
        elif classification['category'] == 'legal_document':
            return {
                **base_analysis,
                'analysis_type': 'Legal Document Analysis',
                'results': {
                    'document_type': 'Legal Document',
                    'key_extracted': random.randint(5, 15),
                    'compliance': random.choice(['Compliant', 'Needs Review', 'Non-Compliant']),
                    'summary': 'Document contains legal terms, contracts, and compliance information for Dubai real estate transactions.',
                    'recommendations': [
                        'Review legal clauses carefully',
                        'Verify all financial terms',
                        'Confirm regulatory compliance'
                    ]
                }
            }
        elif classification['category'] == 'transaction_record':
            return {
                **base_analysis,
                'analysis_type': 'Transaction Data Analysis',
                'results': {
                    'document_type': 'Transaction Records',
                    'records_processed': random.randint(100, 1000),
                    'duplicates_found': random.randint(0, 50),
                    'summary': 'Document contains transaction data with potential duplicate detection and data cleaning.',
                    'recommendations': [
                        'Review duplicate transactions',
                        'Verify data accuracy',
                        'Update transaction records'
                    ]
                }
            }
        elif classification['category'] == 'property_listing':
            return {
                **base_analysis,
                'analysis_type': 'Property Listing Analysis',
                'results': {
                    'document_type': 'Property Listing',
                    'key_extracted': random.randint(5, 15),
                    'compliance': 'N/A',
                    'summary': 'Document contains property listings with details, pricing, and availability information.',
                    'recommendations': [
                        'Review property details',
                        'Verify pricing accuracy',
                        'Update listing status'
                    ]
                }
            }
        elif classification['category'] == 'agent_profile':
            return {
                **base_analysis,
                'analysis_type': 'Agent Profile Analysis',
                'results': {
                    'document_type': 'Agent Profile',
                    'key_extracted': random.randint(3, 10),
                    'compliance': 'N/A',
                    'summary': 'Document contains agent information, credentials, and performance data.',
                    'recommendations': [
                        'Verify agent credentials',
                        'Update contact information',
                        'Review performance metrics'
                    ]
                }
            }
        elif classification['category'] == 'developer_profile':
            return {
                **base_analysis,
                'analysis_type': 'Developer Profile Analysis',
                'results': {
                    'document_type': 'Developer Profile',
                    'key_extracted': random.randint(5, 15),
                    'compliance': 'N/A',
                    'summary': 'Document contains developer information, project portfolio, and company details.',
                    'recommendations': [
                        'Review project portfolio',
                        'Verify company credentials',
                        'Update project status'
                    ]
                }
            }
        else:
            return {
                **base_analysis,
                'analysis_type': 'General Document Analysis',
                'results': {
                    'document_type': 'General Document',
                    'key_extracted': random.randint(3, 10),
                    'compliance': 'N/A',
                    'summary': 'Document contains general real estate information and content.',
                    'recommendations': [
                        'Review document content',
                        'Categorize appropriately',
                        'Extract relevant information'
                    ]
                }
            }
    
    finally:
        # Clean up temporary file
        if temp_file_path.exists():
            temp_file_path.unlink()

@app.get("/uploads/{filename}")
async def get_file(filename: str):
    """Serve uploaded files"""
    file_path = UPLOAD_DIR / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found")
    
    from fastapi.responses import FileResponse
    return FileResponse(file_path)

@app.get("/conversation/{session_id}")
async def get_conversation_by_session(session_id: str):
    """Get conversation and messages by session ID"""
    try:
        with engine.connect() as conn:
            # Get conversation
            result = conn.execute(text("""
                SELECT id, session_id, role, title, created_at, updated_at, is_active
                FROM conversations
                WHERE session_id = :session_id AND is_active = TRUE
            """), {"session_id": session_id})
            
            row = result.fetchone()
            if not row:
                # Create new conversation if not exists
                result = conn.execute(text("""
                    INSERT INTO conversations (session_id, role, title)
                    VALUES (:session_id, :role, :title)
                    RETURNING id, session_id, role, title, created_at, updated_at, is_active
                """), {
                    "session_id": session_id,
                    "role": "client",  # Default role
                    "title": f"Chat Session - {session_id}"
                })
                row = result.fetchone()
                conn.commit()
            
            conversation_id = row[0]
            
            # Get messages for this conversation
            messages_result = conn.execute(text("""
                SELECT id, conversation_id, role, content, timestamp, message_type, metadata
                FROM messages
                WHERE conversation_id = :conversation_id
                ORDER BY timestamp ASC
            """), {"conversation_id": conversation_id})
            
            messages = []
            for msg_row in messages_result:
                messages.append({
                    "sender": "user" if msg_row[2] == "user" else "ai",
                    "text": msg_row[3],
                    "timestamp": str(msg_row[4]),
                    "sources": json.loads(msg_row[6])["sources"] if msg_row[6] and json.loads(msg_row[6]).get("sources") else []
                })
            
            return {
                "conversation_id": conversation_id,
                "session_id": session_id,
                "role": row[2],
                "title": row[3],
                "messages": messages,
                "created_at": str(row[4]),
                "updated_at": str(row[5])
            }
    except Exception as e:
        print(f"Error in get_conversation_by_session: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app, 
        host=HOST, 
        port=PORT,
        log_level=LOG_LEVEL.lower(),
        reload=DEBUG and not IS_PRODUCTION
    )